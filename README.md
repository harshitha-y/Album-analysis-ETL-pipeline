ğŸ¶Album Lyrics Analysis ETL Pipeline

This project is an **end-to-end data engineering pipeline** that extracts, processes, and analyzes song lyrics from **Mac Millerâ€™s albums**.

The pipeline leverages a **modern data stack** to:

* Ingest lyrics from public APIs
* Transform them in the cloud with **AWS Glue & PySpark**
* Load into **Snowflake** for analytics and future visualization

---

## ğŸ›ï¸ Cloud Architecture

The pipeline is built with **layered zones** in an AWS data lake and integrates with Snowflake for scalable analytics.

ğŸ“Œ A full architecture diagram is available in [`architecture/lyrics_etl.png`](architecture/lyrics_etl.png).

Workflow Overview:

1. **Data Extraction (Local)** â€“ Python script fetches track metadata (Spotify API) & lyrics (Genius API).
2. **Landing Zone (S3 Raw)** â€“ Raw JSONL files uploaded to AWS S3 `/raw`.
3. **Schema Discovery (Glue Crawler)** â€“ Infers schema & catalogs data.
4. **ETL Transformation (AWS Glue)** â€“ PySpark cleaning: remove headers, punctuation, extra whitespace.
5. **Processed Zone (S3)** â€“ Outputs structured **Parquet files** to `/processed`.
6. **Data Warehouse (Snowflake)** â€“ Loads Parquet into analytics tables for **SQL queries**.

---

## ğŸ› ï¸ Tech Stack

* Data Ingestion â†’ Python, Requests, BeautifulSoup4
* Cloud â†’ AWS (S3, Glue, IAM)
* ETL & Catalog â†’ AWS Glue Crawlers + PySpark ETL Jobs
* Data Warehouse â†’ Snowflake
* BI (Future) â†’ Amazon QuickSight, Tableau

---

## ğŸ“ Project Structure

```bash
/
â”œâ”€â”€ architecture/               # Architecture diagram
â”‚   â””â”€â”€ lyrics_etl.png
â”œâ”€â”€ data/                       # Raw & processed data (local)
â”œâ”€â”€ docs/                       # Setup guides
â”‚   â”œâ”€â”€ glue_crawler_setup.md
â”‚   â”œâ”€â”€ iam_roles_setup.md
â”‚   â””â”€â”€ snowflake_s3_integration.md
â”œâ”€â”€ glue_jobs/                  # PySpark ETL scripts
â”‚   â””â”€â”€ clean_lyrics.py
â”œâ”€â”€ scripts/                    # Local ingestion scripts
â”‚   â””â”€â”€ 1_fetch_lyrics.py
â”œâ”€â”€ snowflake/                  # SQL scripts for warehouse
â”‚   â”œâ”€â”€ 1_setup_integration.sql
â”‚   â””â”€â”€ 2_load_data.sql
â”œâ”€â”€ .env.example                # Env variable template
â”œâ”€â”€ requirements.txt            # Python dependencies
â””â”€â”€ README.md
```

---

## ğŸš€ Setup & Usage

### 1ï¸âƒ£ Prerequisites

* Python **3.8+**
* AWS account with **S3 & Glue IAM permissions**
* Snowflake account
* API credentials: **Spotify Developer**, **Genius API**

### 2ï¸âƒ£ Local Setup

```bash
git clone <your-repository-url>
cd music-sentiment-pipeline
pip install -r requirements.txt
```

### 3ï¸âƒ£ Configure Environment Variables

```bash
cp .env.example .env
# Fill in your API keys and secrets
```

### 4ï¸âƒ£ Run the Pipeline

1. Run local ingestion â†’ `python scripts/1_fetch_lyrics.py`
2. Upload raw files â†’ S3 `/raw` zone
3. Run Glue Crawler â†’ catalog schema
4. Run Glue ETL job â†’ `glue_jobs/clean_lyrics.py`
5. Load to Snowflake â†’ run SQL scripts in `snowflake/`

---

## ğŸ“„ Documentation

* [`docs/iam_roles_setup.md`](docs/iam_roles_setup.md) â†’ IAM roles & permissions
* [`docs/glue_crawler_setup.md`](docs/glue_crawler_setup.md) â†’ AWS Glue setup
* [`docs/snowflake_s3_integration.md`](docs/snowflake_s3_integration.md) â†’ Snowflake â†” S3 integration

---

## ğŸ”® Future Work

* **Emotion Analysis** â†’ emotion classification using NLP/LLMs
* **Orchestration** â†’ Automate with AWS Step Functions / Apache Airflow
* **Visualization** â†’ BI dashboards
